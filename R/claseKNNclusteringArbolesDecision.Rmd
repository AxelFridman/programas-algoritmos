---
title: "Clasificación"
output:
  html_document:
    df_print: paged
---

# El problema de la clasificación.
## Variable categórica
+ Sí - no
+ Infectado - no infectado
+ Belgrano - Caballito - Almagro - Liniers
+ Venta - alquiler
+ Varón - mujer - no binarix
+ Ceibo - Jacarandá - Lapacho

-------------------------------


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(viridis) # para tener los colores
library(ggplot2) # para tener el alpha para regular opacidad
shapes = c(15, 16, 17) 
shapes <- shapes[as.numeric(iris$Species)]
plot(iris$Sepal.Length, iris$Sepal.Width, col = viridis(3)[iris$Species], xlab = 'Atributo x1', ylab = 'Atributo x2', main = 'Scatterplot por clases', pch = shapes, cex = 0.8)
legend("topleft",
       legend = c('Clase 1', 'Clase 2', 'Clase 3'),
       pch = c(15, 16, 17),
       col =  viridis(3))
```

+ Variables explicativas x1, x2
+ Variable a explicar categórica

-------------------------------

```{r echo=FALSE}
shapes = c(15, 16, 17) 
shapes <- shapes[as.numeric(iris$Species)]
plot(iris$Sepal.Length, iris$Sepal.Width, col = viridis(3)[iris$Species], xlab = 'Atributo x1', ylab = 'Atributo x2', main = 'Scatterplot por clases', pch = shapes, cex = 0.8)
legend("topleft",
       legend = c('Clase 1', 'Clase 2', 'Clase 3'),
       pch = c(15, 16, 17),
       col =  viridis(3))
points(c(6.2), c(3.2), pch = 1, col = "coral") 
```

## ¿Qué clase le asignamos a la nueva observación?

-------------------------------

# Métodos de clasificación

+ A partir de los atributos (variables explicativas) determinan la etiqueta en la variable categórica Y.
+ Aprendizaje supervisado: contamos con un conjunto de entrenamiento en el cual conocemos las etiquetas - valores de la variable Y (a diferencia de Clustering).
+ Evaluación del modelo: relacionada con la cantidad de elementos bien o mal clasificados.
+ En el caso de las variables sí/no, hablamos de falsos positivos, o falsos negativos.

-------------------------------

# Métodos posibles

+ Frontera (x1 > c)
+ Regresión logística
+ Decision trees
+ Support Vector Machines (SVM)
+ K-Nearest Neighbors (KNN)

-------------------------------

## Dataset de flores - Iris
### Fisher - 1936

50 muestras de cada una de tres especies de flores _Iris: setosa, versicolor y virginica_.
De cada flor se midieron 4 atributos: largo y ancho del sépalo y del pétalo.


```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)

```

```{r}
iris
```

-------------------------------

## Veamos algunos métodos de clasificación aplicados a este dataset.
## Después van a tener que replicar este análisis en otro dataset.

*Vamos a necesitar algunos paquetes. E iniciamos una semilla para poder replicar los experimentos.*


```{r echo=TRUE, warning=FALSE, message=FALSE}

set.seed(111) # para poder replicar
```

-------------------------------
Ahora exploremos el dataset Iris (que viene con R)

```{r}
summary(iris)
```


```{r}
iris
```

-------------------------------

Veamos cómo son los datos. Tenemos 4 atributos. Largo y ancho de sépalo y de pétalo. 

Miremos cómo se distribuyen, en cada clase, los valores del largo del Pétalo.

```{r}

par(mfrow = c(2, 2)) 


b = seq(0,8, 0.1)
colores = viridis(3)
hist(iris$Petal.Length[iris$Species == 'setosa'], breaks = b,  xlim = c(0,8), col = alpha(colores[1],0.5), ylim = c(0,15), main = '', xlab = 'Petal Length')
hist(iris$Petal.Length[iris$Species == 'versicolor'], breaks = b, add = TRUE, col = alpha(colores[2], 0.5), ylim = c(0, 5), main = '')
hist(iris$Petal.Length[iris$Species == 'virginica'], breaks = b, add = TRUE,  col = alpha(colores[3], 0.5), ylim = c(0, 5), xlim = c(0,8), main = '')

##########
b = seq(0,8, 0.1)
colores = viridis(3)
hist(iris$Petal.Width[iris$Species == 'setosa'], breaks = b,  xlim = c(0,8), col = alpha(colores[1],0.5), ylim = c(0,15), main = '', xlab = 'Petal Width')
hist(iris$Petal.Width[iris$Species == 'versicolor'], breaks = b, add = TRUE, col = alpha(colores[2], 0.5), ylim = c(0, 5), main = '')
hist(iris$Petal.Width[iris$Species == 'virginica'], breaks = b, add = TRUE,  col = alpha(colores[3], 0.5), ylim = c(0, 5), xlim = c(0,8), main = '')


legend('topright',  legend = unique(iris$Species), 
       fill = viridis(3), title="Clases")

##########
b = seq(0,8, 0.1)
colores = viridis(3)
hist(iris$Sepal.Length[iris$Species == 'setosa'], breaks = b,  xlim = c(0,8), col = alpha(colores[1],0.5), ylim = c(0,15), main = '', xlab = 'Sepal Length')
hist(iris$Sepal.Length[iris$Species == 'versicolor'], breaks = b, add = TRUE, col = alpha(colores[2], 0.5), ylim = c(0, 5), main = '')
hist(iris$Sepal.Length[iris$Species == 'virginica'], breaks = b, add = TRUE,  col = alpha(colores[3], 0.5), ylim = c(0, 5), xlim = c(0,8), main = '')

##########
b = seq(0,8, 0.1)
colores = viridis(3)
hist(iris$Sepal.Width[iris$Species == 'setosa'], breaks = b,  xlim = c(0,8), col = alpha(colores[1],0.5), ylim = c(0,15), main = '', xlab = 'Sepal Width')
hist(iris$Sepal.Width[iris$Species == 'versicolor'], breaks = b, add = TRUE, col = alpha(colores[2], 0.5), ylim = c(0, 5), main = '')
hist(iris$Sepal.Width[iris$Species == 'virginica'], breaks = b, add = TRUE,  col = alpha(colores[3], 0.5), ylim = c(0, 5), xlim = c(0,8), main = '')
```

-------------------------------

### Parece que el largo del pétalo puede servir para nuestra clasificación.

```{r}
clasificador_iris <- function(datos){
  pet_len = datos[3]
  if(pet_len <= 2.5){
    clase = 'setosa'
  }
  else if(pet_len <= 4.5){
    clase = 'versicolor'
  }
  else{
    clase = 'virginica'
  }
  return(clase)
}
```

Entonces por ejemplo podemos ver una fila y ver qué clase se le asigna con nuestro clasificador.

```{r}
flor = iris[122, -5] # le saco la quinta columna, porque el clasificador toma las primeras cuatro
clase_flor = iris$Species[122]
clase_asignada_flor = clasificador_iris(flor)
print(flor)
print(clase_flor)
print(clase_asignada_flor)
```


-------------------------------

### Ahora veamos cómo se comporta este clasificador.

Éstas son las líneas de corte.

```{r}
b = seq(0,8, 0.1)
colores = viridis(3)
hist(iris$Petal.Length[iris$Species == 'setosa'], breaks = b,  xlim = c(0,8), col = alpha(colores[1],0.5), ylim = c(0,15), main = 'Histograma de Petal Length por variedad', xlab = 'Petal Length')
hist(iris$Petal.Length[iris$Species == 'versicolor'], breaks = b, add = TRUE, col = alpha(colores[2], 0.5), ylim = c(0, 5), main = '')
hist(iris$Petal.Length[iris$Species == 'virginica'], breaks = b, add = TRUE,  col = alpha(colores[3], 0.5), ylim = c(0, 5), xlim = c(0,8), main = '')
legend('topright',  legend = unique(iris$Species), 
       fill = viridis(3), title="Clases")
abline(v = 2.5, col = "coral")
abline(v = 4.5, col = "coral")
```

-------------------------------

Y por lo tanto ésta es la clasificación que estamos haciendo.

```{r}
nuevo_iris <- iris
nuevo_iris$clase_asig = apply(nuevo_iris[, -5], 1, function(x) clasificador_iris(x))
nuevo_iris
```

Veamos gráficamente cómo fue la clasificación.

```{r}
b = seq(0,8, 0.1)
colores = viridis(3)
hist(nuevo_iris$Petal.Length[nuevo_iris$clase_asig == 'setosa'], breaks = b,  xlim = c(0,8), col = alpha(colores[1],0.5), ylim = c(0,15), main = 'Histograma de Petal Length por variedad asignada', xlab = 'Petal Length')
hist(nuevo_iris$Petal.Length[nuevo_iris$clase_asig == 'versicolor'], breaks = b, add = TRUE, col = alpha(colores[2], 0.5), ylim = c(0, 5), main = '')
hist(nuevo_iris$Petal.Length[nuevo_iris$clase_asig == 'virginica'], breaks = b, add = TRUE,  col = alpha(colores[3], 0.5), ylim = c(0, 5), xlim = c(0,8), main = '')
legend('topright',  legend = unique(nuevo_iris$clase_asig), 
       fill = viridis(3), title="Clases asignadas")
abline(v = 2.5, col = "coral")
abline(v = 4.5, col = "coral")

```

------------------

### ¿Cómo evaluamos nuestro clasificador?

## Matriz de confusión.

Para cada clase i, nos fijamos cuántas observaciones de la clase fueron clasificadas en cada clase j. Esto nos da una matriz cuadrada, con una fila y columna por cada clase.


```{r}
clases = unique(iris$Species)
matriz_confusion <- matrix(NA, 3, 3)
dimnames(matriz_confusion) <- list(clases, clases)

for(i in 1:3){
  for(j in 1:3){
    filtro = (nuevo_iris$Species == clases[i] & nuevo_iris$clase_asig == clases[j])
    cuenta = nrow(nuevo_iris[filtro,])
    matriz_confusion[i, j] = cuenta
  } 
}
matriz_confusion
```

--------------------------

### Otra forma de evaluar es con la exactitud o _accuracy_ que es una medida numérica que cuenta la proporción de observaciones bien clasificadas.


```{r}
exactitud <- mean(nuevo_iris$Species == nuevo_iris$clase_asig)
exactitud
```

### Bueno, anda bastante bien. ¿Se podría mejorar? ¿El corte que elegimos en 4.5 es el mejor?

Podemos ver qué pasaría con otros valores, y mirar la exactitud en cada uno.

```{r}
exactitud <- function(clasif, data, labels){
  predichos = apply(data, 1, clasif)
  cuenta = mean(predichos == labels)
  return(cuenta)
}

```

Esta función se usa así:

```{r}
exactitud(clasificador_iris, iris[,-5], iris$Species)
```
Entonces ahora miremos, en una grilla entre 4 y 6, los posibles cortes, y qué exactitud implica cada uno.


```{r}
posibles_cortes = seq(4, 6, 0.1)
resultados_exact = c()
for(c in posibles_cortes){
  clasificador_iris_c <- function(datos){
    pet_len = datos[3]
    if(pet_len <= 2.5){
      clase = 'setosa'
    }
    else if(pet_len <= c){
      clase = 'versicolor'
    }
    else{
      clase = 'virginica'
    }
    return(clase)
  }
  exactitud_c = exactitud(clasificador_iris_c, iris[,-5], iris$Species)
  resultados_exact = c(resultados_exact, exactitud_c)
}
```

Ahora graficamos la exactitud en función del corte.

```{r}
plot(posibles_cortes, resultados_exact, pch = 19, cex = 0.5, main = 'Exactitud en función del corte elegido', xlab = 'Corte', ylab = 'Exactitud')
lines(posibles_cortes, resultados_exact)
```

Miremos más de cerca,

```{r}
plot(posibles_cortes[5:15], resultados_exact[5:15], pch = 19, cex = 0.5, main = 'Exactitud en función del corte elegido', xlab = 'Corte', ylab = 'Exactitud')
lines(posibles_cortes[5:15], resultados_exact[5:15])
```

Se ve en este gráfico que los mejores cortes son 4.7 a 4.85.
Podríamos haber calculado este valor analíticamente.


```{r}
i = which.max(resultados_exact)
posibles_cortes[i]
```


-----------------------

# K Nearest Neigbors (KNN)


+ Definir una distancia
+ Buscar los k vecinos más cercanos
+ Ver qué clases tienen
+ Elegir la mayoritaria

### Por ejemplo, tomando la distancia euclídea en los atributos Sepal Length y Sepal Width

+ Buscamos el vecino más cercano, entre los que ya tenemos etiquetados.
+ Nos copiamos esa etiqueta.


```{r echo=FALSE, message=FALSE, warning=FALSE}
shapes = c(15, 16, 17) 
shapes <- shapes[as.numeric(iris$Species)]
plot(iris$Sepal.Length, iris$Sepal.Width, col = viridis(3)[iris$Species], xlab = 'Sepal Length', ylab = 'Sepal Width', main = 'Scatterplot de Sepal length y width por variedad', pch = shapes, cex = 0.8)
legend("topleft",
       legend = unique(iris$Species),
       pch = c(15, 16, 17),
       col =  viridis(3))
points(c(5.75), c(2.5), pch = 19, col = "coral")  
points(c(5.75), c(2.5), pch = 1, cex = 3.5, col = "black")
text(c(6), c(2.5), 'k = 1')
```

------------------------------

+ Buscamos los 3 vecinos más cercanos.
+ Elegimos la etiqueta más frecuente.


```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(iris$Sepal.Length, iris$Sepal.Width, col = viridis(3)[iris$Species], xlab = 'Sepal Length', ylab = 'Sepal Width', main = 'Scatterplot de Sepal length y width por variedad', pch = shapes, cex = 0.8)
legend("topleft",
       legend = unique(iris$Species),
       pch = c(15, 16, 17),
       col =  viridis(3))
points(c(5.75), c(2.5), pch = 19, col = "coral")  
points(c(5.75), c(2.5), pch = 1, cex = 5.1, col = "black")  
text(c(6), c(2.5), 'k = 3')

```

------------------------------

+ Buscamos los 5 vecinos más cercanos.
+ Elegimos la etiqueta más frecuente.


```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(iris$Sepal.Length, iris$Sepal.Width, col = viridis(3)[iris$Species], xlab = 'Sepal Length', ylab = 'Sepal Width', main = 'Scatterplot de Sepal length y width por variedad', pch = shapes, cex = 0.8)
legend("topleft",
       legend = unique(iris$Species),
       pch = c(15, 16, 17),
       col =  viridis(3))
points(c(5.75), c(2.5), pch = 19, col = "coral")  
points(c(5.75), c(2.5), pch = 1, cex = 8.6, col = "black")  
text(c(6.1), c(2.5), 'k = 5')
```


------------------------------

### Ejemplos, usando todo el dataset, con distintos valores de k. 
### Vamos a usar los 4 atributos (4 primeras columnas del dataframe).

Vamos a usar la función `knn` del paquete class.
KNN toma siempre train y test. Podemos empezar usando train = test.

```{r}
library(class) # para el clasificador knn

train =  iris[, -5] # saco la columna con la etiqueta
test =  iris[, -5]

labels_train = iris[,5] # guardo las etiquetas
clasif_iris <- knn(train = train, test = test, cl = labels_train, k = 1)
summary(clasif_iris)
```

```{r}
clasif_iris <- knn(train = train, test = test, cl = labels_train, k = 3)
summary(clasif_iris)
```


```{r}
clasif_iris <- knn(train = train, test = test, cl = labels_train, k = 5)
summary(clasif_iris)
```

```{r}
clasif_iris <- knn(train = train, test = test, cl = labels_train, k = 10)
summary(clasif_iris)
```
### Todos andan muy bien. Veamos las matrices de confusion. Hay una función para esto!
Usamos la función `confusionMatrix` del paquete caret.
```{r}
install.packages("caret")
```

```{r}
library(caret) # para la confusionMatrix

labels_test = iris[,5]
confusionMatrix(clasif_iris, labels_test)
```

Vemos que nos genera la matriz y otra info. Entre otras cosas, la accuracy (exactitud).
Para acceder directamente a este valor, podemos usar:

```{r}
confusionMatrix(clasif_iris, labels_test)$overall[[1]]
```


-------------------------

## ¿Cuál es el mejor valor de k?

Ahora podríamos evaluar los distintos valores de k haciendo cross-validation.

```{r}
Nrep = 20
Ntest = 30
max_val = 50
valores = 1:max_val

resultados_crossval_train = matrix(NA, length(valores), Nrep)
resultados_crossval_test = matrix(NA, length(valores), Nrep)

for(n in 1:Nrep){
  
  indices = sample(seq(1, nrow(iris)), Ntest)
  test = iris[indices, -5]
  train = iris[-indices, -5]
  test_labels = iris$Species[indices]
  train_labels = iris$Species[-indices]
  
  for(k in 1:max_val){
    clasificador_iris_k_train <- knn(train = train, test = train, cl = train_labels, k = k)
    clasificador_iris_k_test <- knn(train = train, test = test, cl = train_labels, k = k)

    exactitud_train = confusionMatrix(clasificador_iris_k_train, train_labels)$overall[[1]]
    exactitud_test = confusionMatrix(clasificador_iris_k_test, test_labels)$overall[[1]]
    
    resultados_crossval_train[k, n] = exactitud_train
    resultados_crossval_test[k, n] = exactitud_test
  }
}
exact_train = rowMeans(resultados_crossval_train)
exact_test = rowMeans(resultados_crossval_test)
```


Graficamos ambas curvas.

```{r}
plot(valores, exact_train, col = 'darkblue', pch = 19, cex = 0.5, xlab = 'Valor de k', ylab = 'Exactitud', main = 'Exactitud en función del k')
lines(valores, exact_train, col = 'darkblue')
points(valores, exact_test, col = 'darkorange', pch = 19, cex = 0.5)
lines(valores, exact_test, col = 'darkorange')
legend('topright', legend = c('train', 'test'), col = c('darkblue', 'darkorange'), pch = 19)
```


```{r}
mejor_k = which.max(exact_test)
mejor_k
```


# Árboles de decisión - Decision Trees

Métodos de umbral sucesivos.

+ Si x1 > 2.5 --> clase_asig = clase 1
+ Si no, si x2 < 6 --> clase_asig= clase 2
+ Si no, clase = clase_asig 3

En R tenemos una función para armar árboles de decisión. `rpart` del paquete `rpart`

```{r}
install.packages("rpart.plot")
```

```{r}
library(rpart)
library(rpart.plot)
```

Separamos train-test y vemos cómo funciona.

```{r}
indices = sample(seq(1:150), 30)
train = iris[-indices,]
test = iris[indices,-5]
fit <- rpart(Species~., data = train, method = 'class')
```

La función `rpart.plot` nos muestra el árbol que se armó.

```{r}
rpart.plot(fit)
```

Para predecir en los datos test usamos `predict`.

```{r}
predichos = predict(fit, newdata = test, type = "class")
```

Y para evaluar, la `confusionMatrix`

```{r}
confusionMatrix(predichos, iris[indices,5])
```


Podemos acotar la profundidad máxima del árbol, con el parámetro maxdepth.

```{r}
fit <- rpart(Species~., data = train, method = 'class', maxdepth = 1)
rpart.plot(fit)
predichos = predict(fit, newdata = test, type = "class")
acc = round(confusionMatrix(predichos, iris[indices,5])$overall[[1]], 3)
legend('topleft', legend = paste('Exactitud', as.character(acc)))

```

Repetimos con profundidad 2.

```{r}
fit <- rpart(Species~., data = train, method = 'class', maxdepth = 2)
rpart.plot(fit)
predichos = predict(fit, newdata = test, type = "class")
acc = round(confusionMatrix(predichos, iris[indices,5])$overall[[1]], 3)
legend('topleft', legend = paste('Exactitud', as.character(acc)))
```

Repetimos con profundidad 3.

```{r}
fit <- rpart(Species~., data = train, method = 'class', maxdepth = 3)
rpart.plot(fit)
predichos = predict(fit, newdata = test, type = "class")
acc = round(confusionMatrix(predichos, iris[indices,5])$overall[[1]], 3)
legend('topleft', legend = paste('Exactitud', as.character(acc)))
```


---------------------------------
# Vamos a aplicar lo visto a un nuevo dataset, de árboles.
## Información sobre árboles de los parques de la ciudad de Buenos Aires. Es un subconjunto de un dataset público.
https://data.buenosaires.gob.ar/dataset/arbolado-espacios-verdes
(pero ahora está funcionando mal el link)

Este dataset lo van a encontrar en el campus.

```{r}
data_arboles <- read.csv('arboles.csv')
```

Veamos cómo son los datos.

```{r}
data_arboles
```
Unidades de medida:

+ La alura es la altura total en metros.
+ El diámetro es el diámetro del tronco a la altura del pecho (de una persona, aprox a 1.3m del piso), y está expresada en centímetros.
+ La inclinación está dada en grados (0 es vertical, 90 es horizontal).

Veamos qué tipos de árboles hay.


```{r}
colnames(data_arboles)
nombres = unique(data_arboles$nombre_com)
nombres
```

```{r}
range(data_arboles$altura_tot)
range(data_arboles$diametro)
range(data_arboles$inclinacio)
```


Podemos graficar, como en el caso anterior, los histogramas de cada clase.


```{r}
#par(mfrow = c(2, 1)) 
b = seq(0,55,1)
l = length(nombres)

colores = viridis(l)
hist(data_arboles$'altura_tot'[data_arboles$nombre_com == 'Ceibo'], breaks = b, col = alpha(colores[1],0.5),  main = 'Histogramas de altura por especie', xlab = 'Altura', probability = TRUE)

for(a in 2:l){
  hist(data_arboles$'altura_tot'[data_arboles$nombre_com == nombres[a]], breaks = b, col =     alpha(colores[a],0.5),  add = TRUE, probability = TRUE)
}
legend('right', legend = unique(data_arboles$nombre_com), col = viridis(l), pch = 19)
```

```{r}
b = seq(0,310, 5)

colores = viridis(l)
hist(data_arboles$diametro[data_arboles$nombre_com == 'Jacarandá'], breaks = b, col = alpha(colores[1],0.5),  main = 'Histogramas de diámetro por especie', xlab = 'Diámetro', probability = TRUE, ylim = c(0, 0.08))

for(a in 2:l){
  hist(data_arboles$'diametro'[data_arboles$nombre_com == nombres[a]], breaks = b, col =     alpha(colores[a],0.5),  add = TRUE, probability = TRUE)
}
legend('right', legend = unique(data_arboles$nombre_com), col = viridis(l), pch = 19)
```

Los histogramas, que muestran una sola variable, nos muestran que no va a ser posible una buena clasificación que considere una sola variable.

```{r}
plot(altura_tot~diametro, data = data_arboles, col = alpha(viridis(l)[nombre_com], 0.3), pch = 19, main = 'Scatterplot de árboles: altura y diámetro por especie', xlab = 'Diámetro', ylab = 'Altura')
legend('right', legend = nombres, col = viridis(l)[nombres], pch = 19)
```



Vamos a utilizar knn.


```{r}
Nrep = 1
Ptest = 0.2 # 20% de los datos para test
max_val = 30
valores = 1:max_val

resultados_crossval_train_arboles = matrix(NA, length(valores), Nrep)
resultados_crossval_test_arboles = matrix(NA, length(valores), Nrep)

atributos = c('altura_tot', 'diametro', 'inclinacio')

for(n in 1:Nrep){
  indices = sample(seq(1, nrow(data_arboles)), Ptest*nrow(data_arboles))
  test = data_arboles[indices, atributos]
  train = data_arboles[-indices, atributos]
  test_labels = data_arboles$nombre_com[indices]
  train_labels = data_arboles$nombre_com[-indices]
  
  for(k in 1:max_val){
    clasif_arbol_train <- knn(train = train, test = train, cl = train_labels, k = k)
    clasif_arbol_test <- knn(train = train, test = test, cl = train_labels, k = k)

    exactitud_train = confusionMatrix(clasif_arbol_train, train_labels)$overall[[1]]
    exactitud_test = confusionMatrix(clasif_arbol_test, test_labels)$overall[[1]]
    
    resultados_crossval_train_arboles[k, n] = exactitud_train
    resultados_crossval_test_arboles[k, n] = exactitud_test
  }
}
exact_train_arbol = rowMeans(resultados_crossval_train_arboles)
exact_test_arbol = rowMeans(resultados_crossval_test_arboles)

```

Graficamos ambas curvas.

```{r}
plot(valores, exact_train_arbol, col = 'darkblue', pch = 19, cex = 0.5, ylim = c(0.7, 0.85), xlab = 'Valor de k', ylab = 'Exactitud', main = 'Exactitud en función del k')
lines(valores, exact_train_arbol, col = 'darkblue')
points(valores, exact_test_arbol, col = 'darkorange', pch = 19, cex = 0.5)
lines(valores, exact_test_arbol, col = 'darkorange')
legend('topright', legend = c('train', 'test'), col = c('darkblue', 'darkorange'), pch = 19)
```

Teniendo en cuenta que estamos utilizando una distancia, vamos a reescalar los datos.

```{r}

data_arboles$altura_esc <- (data_arboles$altura_tot - min(data_arboles$altura_tot))/
  (max(data_arboles$altura_tot) - min(data_arboles$altura_tot))

data_arboles$diametro_esc <- (data_arboles$diametro - min(data_arboles$diametro))/
  (max(data_arboles$diametro) - min(data_arboles$diametro))

data_arboles$inclinacion_esc <- (data_arboles$inclinacio - min(data_arboles$inclinacio))/
  (max(data_arboles$inclinacio) - min(data_arboles$inclinacio))

```



Volvemos a hacer knn con este dataframe.

```{r}
Nrep = 1
Ptest = 0.2 # 20% de los datos para test
max_val = 50
valores = 1:max_val

atributos = c('altura_esc', 'diametro_esc', 'inclinacion_esc')

resultados_crossval_train_arboles = matrix(NA, length(valores), Nrep)
resultados_crossval_test_arboles = matrix(NA, length(valores), Nrep)

for(n in 1:Nrep){
  indices = sample(seq(1, nrow(data_arboles)), Ptest*nrow(data_arboles))
  test = data_arboles[indices, atributos]
  train = data_arboles[-indices, atributos]
  test_labels = data_arboles$nombre_com[indices]
  train_labels = data_arboles$nombre_com[-indices]
  
  for(k in 1:max_val){
    clasif_arbol_train <- knn(train = train, test = train, cl = train_labels, k = k)
    clasif_arbol_test <- knn(train = train, test = test, cl = train_labels, k = k)

    exactitud_train = confusionMatrix(clasif_arbol_train, train_labels)$overall[[1]]
    exactitud_test = confusionMatrix(clasif_arbol_test, test_labels)$overall[[1]]
    
    resultados_crossval_train_arboles[k, n] = exactitud_train
    resultados_crossval_test_arboles[k, n] = exactitud_test
  }
}
exact_train_arbol = rowMeans(resultados_crossval_train_arboles)
exact_test_arbol = rowMeans(resultados_crossval_test_arboles)

```

Graficamos

```{r}
plot(valores, exact_train_arbol, col = 'darkblue', pch = 19, cex = 0.5, ylim = c(0.7, 0.85), xlab = 'Valor de k', ylab = 'Exactitud', main = 'Exactitud en función del k')
lines(valores, exact_train_arbol, col = 'darkblue')
points(valores, exact_test_arbol, col = 'darkorange', pch = 19, cex = 0.5)
lines(valores, exact_test_arbol, col = 'darkorange')
legend('topright', legend = c('train', 'test'), col = c('darkblue', 'darkorange'), pch = 19)
```

Mejoró mucho la clasificación. Se pueden hacer más repeticiones (Nrep) y probar más valores de k.


# Ahora clasificamos con árboles de decisión

```{r}
columnas = c('altura_tot', 'diametro', 'inclinacio', 'nombre_com')
atributos = c('altura_tot', 'diametro', 'inclinacio')

indices = sample(seq(1,nrow(data_arboles)), 0.15*nrow(data_arboles))
train = data_arboles[-indices, columnas]
test = data_arboles[indices, atributos ]
fit <- rpart(nombre_com~., data = train, method = 'class')
rpart.plot(fit)

```

```{r}
predichos = predict(fit, newdata = test, type = "class")
confusionMatrix(predichos, data_arboles$nombre_com[indices])
range(data_arboles$altura_tot)
range(data_arboles$diametro)
```

Probar con distintas prof de árbol.
1. Cargar los paquetes necesarios.

```{r}
library(viridis) # opcional para tener los colores
library(ggplot2) # para tener el alpha para regular opacidad
library(GGally) # para tener el alpha para regular opacidad
library(class) # para el clasificador knn
library(caret) # para la confusionMatrix
library(rpart) # para usar árboles de decisión
library(rpart.plot) # para mostrar árboles de decisión
```


2. Cargar el dataset de árboles.

3. Realizar un análisis exploratorio.
```{r, warning=FALSE, error=FALSE, fig.width=10, fig.height=10}
ggp <- ggpairs(data_arboles,
               columns = c(1,2, 3,4),
               mapping = ggplot2::aes(color = as.factor(data_arboles$nombre_com)))
print(ggp, progress = FALSE)
```


4. Hacer un train-test split y hacer una clasificación con knn. Probar con distintos valores de k.

_Código de ayuda (ver también el de la clase)_
```{r}
cantK = 25
vecesApromediar = 10
erroresTrain = c()
erroresTest = c()
for (i in 1:cantK) {
  erroresDeEsteTestParticular = c()
  erroresDeEsteTrainParticular = c()
  for (j in 1:vecesApromediar) {
    k = i # elegir el k
    Ptest = 0.2 # elegir el porcentaje de datos test
    atributos = c('altura_tot', 'diametro', 'inclinacio')
    
    indices = sample(seq(1, nrow(data_arboles)), Ptest*nrow(data_arboles))
    
    test = data_arboles[indices, atributos]
    train = data_arboles[-indices, atributos]
    
    test_labels = data_arboles$nombre_com[indices]
    train_labels = data_arboles$nombre_com[-indices]
    
    clasif_arbol_train <- knn(train = train, test = train, cl = train_labels, k = k, )
    clasif_arbol_test <- knn(train = train, test = test, cl = train_labels, k = k)
    exactitud_train = confusionMatrix(clasif_arbol_train, train_labels)$overall[[1]]
    exactitud_test = confusionMatrix(clasif_arbol_test, test_labels)$overall[[1]]
    erroresDeEsteTrainParticular[j] = exactitud_train
    erroresDeEsteTestParticular[j] = exactitud_test
  }
  erroresTrain[i] = mean(erroresDeEsteTrainParticular)
  erroresTest[i] = mean(erroresDeEsteTestParticular)
}
```
4. Cross-validation: ajustar el modelo para cada valor de k dentro de un rango, y graficar la exactitud en función del k. Considerar hacer repeticiones en cada caso para filtrar la variabilidad.
```{r}
plot(1:cantK, erroresTrain, ylim = range(erroresTrain, erroresTest))
lines(1:cantK, erroresTrain)
points(1:cantK, erroresTest)
lines(1:cantK, erroresTest)
```




5. Reescalar los atributos para que tomen valores entre 0 y 1 y repetir. ¿Mejora la clasificación?
```{r}
data_arboles$altura_esc = data_arboles$altura_tot / max(data_arboles$altura_tot)
range(data_arboles$altura_esc)
```

6. Utilizar ahora árboles de decisión para la clasificación. En este caso no es necesario usar los datos reescalados. Explorar el parámetro maxdepth y ver los árboles generados.

```{r}
columnas = c('altura_tot', 'diametro', 'inclinacio', 'nombre_com')
atributos = c('altura_tot', 'diametro', 'inclinacio')

indices = sample(seq(1,nrow(data_arboles)), 0.15*nrow(data_arboles))
train = data_arboles[-indices, columnas]
test = data_arboles[indices, atributos ]
fit <- rpart(nombre_com~., data = train, method = 'class', maxdepth = 10)
rpart.plot(fit)

```

```{r}
predichos = predict(fit, newdata = test, type = "class")
confusionMatrix(predichos, data_arboles$nombre_com[indices])
```

