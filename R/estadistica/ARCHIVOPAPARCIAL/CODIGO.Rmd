---
title: "R Notebook"
output: html_notebook
---

```{r}
library(plyr)
library(readr)
library(dplyr)
library(caret)
library(ggplot2)
library(repr)
library(car)
```

¿Cúales son los coeficientes estimados para α y β usando m ́ınimos cuadrados?
alpha Sombrero = 12.34 y beta Sombrero = 0.00065


b) ¿Hay evidencia suficiente a nivel 0,01 para decir que β  = 0? 
no hay evidencia suficiente ya que p valor es 5.22e-13.

¿Cúal es el esta ́ıstico del test correspondiente y cu ́al es su distribci ́on bajo H0 ? 

(Beta estimado - 0)/( raiz(var(Beta estimado) ) ) tiene distribucion Tstudent n-2 (porque son 2 parametros los que estoy estimando)

¿Cuanto vale este estad́ıstico para estos datos?
12.27

Hallar el p−valor. Interpretar la conclusíon de este test.
5.22e-13 * 2 ya que es la probabilidad de que sea 0 y sea mayor o sea menor que -12.27

c) Considerar las hip ́otesis H0 : α = 10 vs. H1 : α > 10. ¿Hay evidencia suficiente para
rechazar H0 a nivel 0,05? 

(alpha estimado - 10) / sd ~   Tstudent n-2

luego en nuestro caso como alpha estimado es 12.34, 
obtenemos 2.34 / (2.64) = 0.886
luego como qt(0.95, 30) = 1.69 > 0.886 no rechazo h0

Hallar el p−valor.

p-valor seria 1-pt(0.886, 30) 

d ) Hallar un intervalo de confianza para α de nivel 0,95.

[xbarra - tstudent(n-2, 1-alpha/2) * sd;  xbarra + tstudent(n-2, 1-alpha/2) * sd]
como qt(0.975, 30) = 2.04

[12.34 - 2.04 * 2.64; 12.34 + 2.04 * 2.64]

¿Cúanto vale el R2 en este caso y ćomo interpréıa este valor en t́erminos de las variables del enunciado?

el 83% de la variabilidad de tu variable Y  (que mide el tiempo) es explicada por el modelo en función del peso 

e) Llega un nuevo cargamento con peso 1000. ¿C ́omo estimar ́ıa el tiempo medio que se
tarda en descargarlo?
12.34 + 0.00065*1000
= 12.34 + 0.65 = 12.99

```{r}
df = read.csv("glakes.csv")
```

```{r}
mod = lm(data=df, Time~Tonnage)
summary(mod)
y = predict(mod, df)
```
```{r}
plot(df$Tonnage, df$Time)
lines(df$Tonnage, y)
```
```{r}
pt(2.24,30)
```
```{r}
coef = mod$coefficients
s = 2.6426
xt = as.numeric(df$Tonnage)
Test = (t(xt) * coef - t(xt)*10)/(sqrt(s**2 * t(xt) ) )
```
```{r}
A = rnorm(100, 0, 1)
B = rnorm(100, 0.1, 1)
var.test(A, B, alternative = "two.sided") # test para sigma A = sigma B
t.test(A,B, var.equal = TRUE) # test para mu A = mu B
shapiro.test(A) # test de normalidad
```

```{r}
errorCuadraticoMedio = function(y, yhat){
  return(mean((y-yhat)**2))
}
```

```{r}
looLM = function(datos, evaluador){
  errores = c()
  datos$indice = 1:nrow(datos)
  for (i in 1:nrow(datos)){
    train = datos[datos$indice != i,]
    test = datos[datos$indice == i,]
    modelo = lm(data=train, Time ~ Tonnage) 
    yhatTrain =  predict(modelo)
    yhatTest =  predict(modelo, newdata = test )
    errores[i] = evaluador(test$Time, yhatTest)
  }
  return(mean(errores))
}
```



```{r}
looLM(df, errorCuadraticoMedio)
```
EJEMPLO MTCARS
Ridge es alpha = 0,          lasso es alpha =1
```{r}
y <- mtcars$hp

#define matrix of predictor variables
x <- data.matrix(mtcars[, c('mpg', 'wt', 'drat', 'qsec')])
library(glmnet)

#fit ridge regression model
model <- glmnet(x, y, alpha = 0)

#view summary of model
summary(model)
```
```{r}
cv_model <- cv.glmnet(x, y, alpha = 0, lambda=seq(0,19,0.01))

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

#produce plot of test MSE by lambda value
plot(cv_model) 
```
```{r}
best_model <- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_model)
```
```{r}
#produce Ridge trace plot
plot(model, xvar = "lambda")

```
```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = x)

#find SST and SSE
sst <- sum((y - mean(y))^2)
sse <- sum((y_predicted - y)^2)

#find R-Squared
rsq <- 1 - sse/sst
rsq

```
Caso tonnage tiempo
```{r}
y <- df$Time
df$unos = rep(1, nrow(df))
#define matrix of predictor variables
x <- data.matrix(df[, c('Tonnage', "unos")])

#fit ridge regression model
modelRidge <- glmnet(x, y, alpha = 0)
#Lasso
modelLasso <- glmnet(x, y, alpha = 1)

#view summary of model
summary(modelRidge)
summary(modelLasso)
```
```{r}
cv_modelRidge <- cv.glmnet(x, y, alpha = 0, lambda=seq(0.01,20,0.01))
cv_modelLasso <- cv.glmnet(x, y, alpha = 1, lambda=seq(0.01,20,0.01))

#find optimal lambda value that minimizes test MSE
best_lambdaRidge <- cv_modelRidge$lambda.min
best_lambdaLasso <- cv_modelLasso$lambda.min

#produce plot of test MSE by lambda value
plot(cv_modelRidge) 
plot(cv_modelLasso)
print(best_lambdaRidge)
print(best_lambdaLasso)
```



MODELO LINEAL CLASE 
```{r}
# ESTIMAR COPEFICIENTES
#cargo los datos
datos<-cars
names(datos)

#defino las variables
x<-datos$speed #velocidad del auto
y<-datos$dist #distancia requerida de frenado

plot(x,y)

# armamos a mano la matriz de diseño
X<-cbind(rep(1,length(x)),x)

#calculamos usando la formula
beta_sombrero<-solve(t(X)%*%X)%*%t(X)%*%y

beta_sombrero0<-beta_sombrero[1]
beta_sombrero1<-beta_sombrero[2]

#graficamos la recta
plot(x,y,pch=20)
abline(beta_sombrero,col="dodgerblue3", lwd=2)#, lty=2)

#superponemos las estimaciones de nuestros puntos de diseño
y_sombrero<-beta_sombrero0+beta_sombrero1*x
points(x,y_sombrero,col="orange", pch=18)


# estimamos sigma^2

sum((y-y_sombrero)^2)/(length(x)-2)

# para agregar mas covariables

x2<-x^2
reg2<-lm(y~x+x2)
summary(reg2)

#graficamos
y2<-reg2$coefficients[1]+reg2$coefficients[2]*x+reg2$coefficients[3]*x2
lines(x,y2, col="darkolivegreen", lwd=2)
```
```{r}
# Una no lineal
#######################################################
rm(list=ls())


## Simulacion


sigma<-sqrt(1.3)
beta.star0<- 0.1
beta.star1<- 0.025


equis<- seq(1,10, by=0.1)
set.seed(123)

z<- exp(beta.star0+beta.star1*equis*equis)
y<- z+rnorm(length(equis),0,sigma)

# con puntos
plot(equis,y,pch=20,col="grey30",xlab="x",ylim=c(0,10))

#grafico la verdadera
lines(equis,z,lwd=2,col="magenta")


###########################################################
#
#Ajusto un modelo lineal simple 
#
#E(Y)= beta_0+beta_1*equis
#
###########################################################

salida.lineal<- lm(y~equis)
lines(equis,salida.lineal$fit,col="orange",lwd=2)


###########################################################
#
#Ajusto un modelo una cuadratica
#
#E(Y)= beta_0+beta_1*equis+beta_2*equis^2
#
###########################################################

plot(equis,y,pch=20,col="grey30",xlab="x",ylim=c(0,10))
lines(equis,z,lwd=2)

salida<- lm(y~poly(equis,2,raw=TRUE))
lines(equis,salida$fit,col="orange",lwd=2)

###########################################################
#
#Ajusto un modelo un polinomio de grado 5
#
#E(Y)= beta_0+beta_1*equis+...+beta_5*equis^5
#
###########################################################

plot(equis,y,pch=20,col="grey30",xlab="x",ylim=c(0,10))
lines(equis,z,lwd=2)

salida<- lm(y~poly(equis,5,raw=TRUE))
lines(equis,salida$fit,col="orange",lwd=2)

colores<-rainbow(9)

###########################################################
#
# Ahora lo repito 9 veces y grafico las rectas ajustadas
#
###########################################################


# sin puntos
plot(equis,y,pch=20,col="grey30",xlab="x",type="n",ylim=c(0,10))
lines(equis,z,lwd=2)

set.seed(123)
for(i in 1: 9){
  z<- exp(beta.star0+beta.star1*equis*equis)
  y<- z+rnorm(length(equis),0,sigma)
  salida.lineal<- lm(y~equis)
  lines(equis,salida.lineal$fit,col=colores[i],lwd=2)
}
lines(equis,z,lwd=2)
#points(equis,y)
###########################################################
#
# Repito 9 veces y grafico las cuadraticas ajustadas
#
###########################################################

### Ahora ajusto cuadraticas
set.seed(123)

z<- exp(beta.star0+beta.star1*equis*equis)
y<- z+rnorm(length(equis),0,sigma)

plot(equis,y,pch=20,col="grey30",xlab="x",type="n",ylim=c(0,10))
lines(equis,z,lwd=2)

salida<- lm(y~poly(equis,2,raw=TRUE))
lines(equis,salida$fit,col=15,lwd=2)


for(i in 1:9){
  z<- exp(beta.star0+beta.star1*equis*equis)
  y<- z+rnorm(length(equis),0,sigma)
  salida<- lm(y~poly(equis,2,raw=TRUE))
  lines(equis,salida$fit,col=colores[i],lwd=2)
}
lines(equis,z,lwd=2)
#points(equis,y)

###########################################################
#
# 9 veces y grafico los polinomio de grado 5 ajustados
#
###########################################################

set.seed(123)

z<- exp(beta.star0+beta.star1*equis*equis)
y<- z+rnorm(length(equis),0,sigma)

plot(equis,y,pch=20,col="grey30",xlab="x",type="n",ylim=c(-1,10))
lines(equis,z,lwd=2)

salida<- lm(y~poly(equis,5,raw=TRUE))
lines(equis,salida$fit,col="orange",lwd=2)

for(i in 1: 9){
  z<- exp(beta.star0+beta.star1*equis*equis)
  y<- z+rnorm(length(equis),0,sigma)
  salida<- lm(y~poly(equis,5,raw=TRUE))
  lines(equis,salida$fit,col=colores[i],lwd=2)
}

lines(equis,z,lwd=2)


###########################################################
#
# Vamos a predecir con los distintos ajustes en x_o=4
# Repetimos Nrep=1000 veces
#
###########################################################
Nrep<- 1000
set.seed(123)

pred1<-pred2<-pred5<-pred10<-pred15<-c()



equis.new<- data.frame(equis=c(4))

for(i in 1: Nrep){
  z<- exp(beta.star0+beta.star1*equis*equis)
  y<- z+rnorm(length(equis),0,sigma)
  salida1<- lm(y~equis)
  salida2<- lm(y~poly(equis,2,raw=TRUE))
  salida5<- lm(y~poly(equis,5,raw=TRUE))
  salida10<- lm(y~poly(equis,10,raw=TRUE))
  salida15<- lm(y~poly(equis,15,raw=TRUE))
  pred1<-c(pred1,predict(salida1,newdata=equis.new))
  pred2<-c(pred2,predict(salida2,newdata=equis.new))
  pred5<-c(pred5,predict(salida5,newdata=equis.new))
  pred10<-c(pred10,predict(salida10,newdata=equis.new))
  pred15<-c(pred15,predict(salida15,newdata=equis.new))
}

boxplot(pred1,pred2,pred5,pred10,pred15,names=c("1","2","5","10","15"),xlab="Grado del Polinomio")
abline(h=exp(beta.star0+beta.star1*4*4),lwd=2,col="magenta")
       
       
       
       
```

```{r}
nbp <- read.table("bajoPeso.txt",header=TRUE)
names(nbp)
m <- dim(nbp)
attach(nbp)
plot(apgar5,presSist)
ajuste <- lm(presSist~edadG+apgar5)
names(ajuste)
summary(ajuste)
names(summary(ajuste))
summary(ajuste)$sigma
sum(ajuste$coefficients*c(1,31,7))
```

REGULARIZACION
```{r}
install.packages("ISLR")
install.packages("plotmo")     # para graficar
install.packages("glmnet")
install.packages("regclass")
```

```{r}
rm(list=ls())


#cargo librerias
library(ISLR)
library(plotmo)     # para graficar
library(glmnet)

#cargo datos

#fix(Hitters)
names(Hitters)
dim(Hitters)


#Chequeo missings
sum(is.na(Hitters$Salary))

Hitters<- na.omit(Hitters)
sum(is.na(Hitters$Salary))
dim(Hitters)

# Ajustamos minimos cuadrados con todas las covariables
summary(lm(Salary~.,Hitters))


#Creamos la muestra de entrenamiento y de validacion
set.seed(1)
train <- sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test <- (!train )

#Creo la matriz de dise�o y vector de respuestas 
x <- model.matrix(Salary~.,Hitters)[,-1]
y <- Hitters$Salary
y.test=y[test]

# Calculo LASSO
grid<- 10^seq(10,-2,length=100)
lasso.mod =glmnet (x[train ,],y[train],alpha =1, lambda =grid) #por default estandariza variables

## Veo los coeficientes
dim(coef(lasso.mod))

lasso.mod$lambda

#Inpeccionamos para distintos valores de lambda
lasso.mod$lambda[90]
coef(lasso.mod)[,90]

lasso.mod$lambda[70]
coef(lasso.mod)[,70]

lasso.mod$lambda[30]
coef(lasso.mod)[,30]

## Grafico 
plot(lasso.mod)

plot(lasso.mod,label=T,xvar="lambda")

#Creo la matriz de dise�o y vector de respuestas 
x <- model.matrix(Salary~.,Hitters)[,-1]
y <- Hitters$Salary
y.test=y[test]

#LASSO
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha =1)
plot(cv.out)
bestlam <- cv.out$lambda.min
log(bestlam)

#miro cuanto vale el MSE en la muestra de validacion
coef(lasso.mod,s=cv.out$lambda.min)
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)

#Tmb tenemos el log de la regla de 1 desvio standard
log(cv.out$lambda.1se)
coef(lasso.mod,s=cv.out$lambda.1se)

lasso.pred=predict(lasso.mod,s=cv.out$lambda.1se,newx=x[test,])
mean((lasso.pred-y.test)^2)

###############################
##Comparamos con Ridge
# Calculo Ridge

grid<- 10^seq(10,-2,length=100)
ridge.mod =glmnet (x[train ,],y[train],alpha =0, lambda =grid) #por defalut estandariza variables
plot(ridge.mod)

plot(ridge.mod,label=T,xvar="lambda")

#Ridge
set.seed(1)
cv.out <- cv.glmnet(x[train,],y[train],alpha =0)
plot(cv.out)
bestlam <- cv.out$lambda.min
log(bestlam)

#miro cuanto vale el MSE n la muestra de validacion
coef(ridge.mod,s=cv.out$lambda.min)

ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)

```

```{r}
rdpercentGDP <- read.csv("rdpercentGDP.csv",sep = "")
attach(rdpercentGDP)
names(rdpercentGDP)
head(rdpercentGDP)
par(mfrow=c(1,1))

plot(1996:2016,france,type="l",col="blue",ylim=c(0,3.8),xlab="years",ylab="R&D",
     main="Research and development expenditure (% of GDP)",lwd=2)
points(1996:2016,usa,type="l",lwd=2)
points(1996:2016,argen,type="l",col="lightblue",lwd=2)
points(1996:2016,ger,type="l",col="orange",lwd=2)
points(1996:2016,china,type="l",col="red",lwd=2)
points(1996:2016,japan,type="l",col="green",lwd=2)
points(1996:2016,finl,type="l",col="magenta",lwd=2)
points(1996:2016,uk,type="l",col="grey",lwd=2)

text(2015.2,japan[21]+0.3,labels="Japan",col="green")
text(2015.2,france[21]+0.15,labels="France",col="blue")
text(2015,ger[21]+0.1,labels="Germany",col="orange")
text(2015.8,usa[21]-0.1,labels="USA",col="black")
text(2015.2,china[21]-0.2,labels="China",col="red")
text(2014.8,argen[21]+0.2,labels="Argentina",col="lightblue")
text(2015.5,uk[21]-0.2,labels="UK",col="grey")
text(2012,finl[17]+0.4,labels="Finland",col="magenta")

# Estudiemos un modelo de regresion
set.seed(27)
entrena <- sample(c(TRUE,FALSE),nrow(rdpercentGDP),rep=TRUE,prob=c(0.8,0.2)) 

ajusteml<-lm(usa~.,data = rdpercentGDP[entrena,-1])
summary(ajusteml) #no dan significativas todas...

# Vemos que cambiando la semilla me da re diferente la estimacion
# será que esta todo super correlacionad??

XX <- model.matrix(ajusteml)[,-1] #sin el intercept

cor(rdpercentGDP[2:9]) #vemos que hay covariables que estan muy correlacionadas :(

# bonus track
# ¿Estan correlacionadas las variables?
#library(regclass)
#VIF(ajusteml)#vif>10 alta correlacion

#ahora con Ridge, que es la penalizaci�n que se recomienda para estos casos

# poner alpha = 0 para que haga el ajuste ridge
ajuste.ridge <- glmnet(XX,usa[entrena],alpha=0)    
names(ajuste.ridge)
summary(ajuste.ridge) ## no se entiende mucho
plot(ajuste.ridge,label = T,xvar="lambda",lwd=2,main="Ridge Regression")
abline(h=0,lty=2)
ajuste.ridge$lambda
coef.glmnet(ajuste.ridge) ## coeficientes para valores de lambda en ajuste.ridge$lambda
grilla <- 10^seq(10,-2,length=100)
### elijo lambda por validaci�n cruzada
set.seed(21)
vcR <- cv.glmnet(XX,usa[entrena],alpha=0,lambda=grilla,nfolds = 5)
plot(vcR)
names(vcR)
vcR$lambda.min
vcR$lambda.1se
rid.1se<-glmnet(XX,usa[entrena],alpha=0,lambda = vcR$lambda.1se)
rid.min<-glmnet(XX,usa[entrena],alpha=0,lambda = vcR$lambda.min)
coef(rid.1se)
coef(rid.min)
ajusteml

## poner alpha=1 si queremos hacer LASSO
set.seed(87)
vcL <- cv.glmnet(XX,usa[entrena],alpha=1,lambda=grilla,nfolds = 5)
plot(vcL)
vcL$lambda.min
vcL$lambda.1se
lasso.1se<-glmnet(XX,usa[entrena],alpha=1,lambda = vcL$lambda.1se)
lasso.min<-glmnet(XX,usa[entrena],alpha=1,lambda = vcL$lambda.min)
coef(lasso.1se) 
coef(lasso.min) ## selecciona mas variables

### medimos la capacidad de predicci�n
prueba <-(!entrena)
is.matrix(rdpercentGDP[prueba,-c(1,9)]) ## no consideramos a la primera y última columna
## el as.matrix es un comando que "salva" cuando se necesita objeto matriz
nuevo <- as.matrix(rdpercentGDP[prueba,-c(1,9)]) ## me quedo con la submatriz que contiene
## a las filas de prueba de las covariables involucradas

## Ridge
ridge.pred <- predict(rid.1se,s=vcR$lambda.1se,newx=nuevo)
ecmR <- mean((ridge.pred-usa[prueba])^2)
## con todo el conjunto de datos, estimo los par�metros
covariables <- as.matrix(rdpercentGDP[,-c(1,9)])
salidaR <- glmnet(covariables,usa,alpha=0)
predict(salidaR,type="coefficients",s=vcR$lambda.1se)[1:8,]
## LASSO
lasso.pred <- predict(lasso.1se,s=vcL$lambda.1se,newx=nuevo)
ecmL <- mean((lasso.pred-usa[prueba])^2)
## ahora con todo el conjunto de datos, estimo los par�metros
salidaL <- glmnet(covariables,usa,alpha=1)
predict(salidaL,type="coefficients",s=vcL$lambda.1se)[1:8,]

c(ecmR,ecmL) ## da mejor error de predicci�n ridge

```

```{r}
library(ggplot2)

#cargo los datos
autos<-cars

#defino las variables
x<-autos$speed #velocidad del auto
y<-autos$dist #distancia requerida de frenado

plot(x,y,pch=20)


#grafico bonitos
g2<-ggplot(data=autos,aes(x=speed, y=dist))+ 
  geom_point()+
  labs(title="",
       x="Velocidad", 
       y = "Distancia de frenado")+
  theme_light()
g2

reg<-lm(dist~speed,data=autos)
summary(reg)

#ic para los coeficientes de la regresion

LI<-summary(reg)$coef[2,1]-qt(0.975,32-2)*summary(reg)$coef[2,2]
LS<-summary(reg)$coef[2,1]+qt(0.975,32-2)*summary(reg)$coef[2,2]

c(LI,LS)

confint(reg)

#Una forma de graficarlo sin hacer las cuentas, con ggplot2
g2+ geom_smooth(method="lm", col="firebrick",se=FALSE)+
  labs(title="",
       x="Velocidad", 
       y = "Distancia de frenado")+
  theme_light()


# Calculamos los intervalos usando la fórmula

# a) Intervalos de CONFIANZA
n<-nrow(autos)
X<-model.matrix(reg)
p<-ncol(X) #cant de parametros a estimar

res<-reg$residuals # los residuos son y - y^

s2<-t(res)%*%res/(n-p)
# s2<-summary(reg)$sigma^2 si no quiero hacer la cuenta

A<-solve(t(X)%*%X)



#Podemos hacer la cuenta a mano... yo^=beta0+beta1*x0

IC<-matrix(0,nrow = n,ncol=2)
for(i in 1:n)
{
  IC[i,]<-c(reg$fitted.values[i]-qt(0.975,n-p)*sqrt(s2[1,1]*t(X[i,])%*%A%*%X[i,]),
            reg$fitted.values[i]+qt(0.975,n-p)*sqrt(s2[1,1]*t(X[i,])%*%A%*%X[i,])) 
}


# O dejamos que lo haga R

int<-predict(reg ,interval = "confidence", level = 0.95)


# b) Intervalos de PREDICCION

ICP<-matrix(0,nrow = n,ncol=2)
for(i in 1:n)
{
  ICP[i,]<-c(reg$fitted.values[i]-qt(0.975,n-p)*sqrt(s2[1,1]*(1+t(X[i,])%*%A%*%X[i,])),
             reg$fitted.values[i]+qt(0.975,n-p)*sqrt(s2[1,1]*(1+t(X[i,])%*%A%*%X[i,]))) 
}


# o con R

intP<-predict(reg,interval = "prediction", level = 0.95)


# graficamos con plot

plot(x,y,pch=20)
abline(reg, col=col1, lwd=2)
points(x,IC[,1],pch=20,col=col2)
points(x,IC[,2],pch=20,col=col2)
points(x,ICP[,1],pch=20,col=col3)
points(x,ICP[,2],pch=20,col=col3)
# o con ggplot2

# Creamos un nuevo data frame con toda la información

intervalos<-data.frame(cbind(autos,IC,ICP))

g<-ggplot(intervalos)+
  geom_point(aes(x=speed,y=dist))+
  geom_line(aes(x=speed,y=X1), col="skyblue")+
  geom_line(aes(x=speed,y=X2), col="skyblue")+
  geom_line(aes(x=speed,y=X1.1), col="chocolate",lty=4)+
  geom_line(aes(x=speed,y=X2.1), col="chocolate",lty=4)+
  geom_smooth(aes(x=speed,y=dist),method="lm", col="firebrick",se=FALSE)+
  theme_light()
g



# Bandas de confianza

BC<-matrix(0,nrow = n,ncol=2)
for(i in 1:n)
{
  BC[i,]<-c(reg$fitted.values[i]-sqrt(p*qf(0.95,p,n-p))*sqrt(s2[1,1]*(t(X[i,])%*%A%*%X[i,])),
            reg$fitted.values[i]+sqrt(p*qf(0.95,p,n-p))*sqrt(s2[1,1]*(t(X[i,])%*%A%*%X[i,]))) 
}

intervalos<-data.frame(cbind(autos,IC,ICP,BC))
g<-ggplot(intervalos)+
  geom_point(aes(x=speed,y=dist))+
  geom_line(aes(x=speed,y=X1), col="skyblue")+
  geom_line(aes(x=speed,y=X2), col="skyblue")+
  geom_line(aes(x=speed,y=X1.1), col="chocolate",lty=4)+
  geom_line(aes(x=speed,y=X2.1), col="chocolate",lty=4)+
  geom_line(aes(x=speed,y=X1.2), col="chartreuse4",lwd=1.5)+
  geom_line(aes(x=speed,y=X2.2), col="chartreuse4",lwd=1.5)+
  geom_smooth(aes(x=speed,y=dist),method="lm", col="firebrick",se=FALSE)+
  theme_light()
g

confint(reg)
```
```{r}

```



```{r}
dfTrain = read.table("alturasEntre.txt", header = F)
dfTest = read.table("alturasPrueba.txt", header = F)
colnames(dfTrain) = c("altura","sexo")
colnames(dfTest) = c("altura","sexo")
```
```{r}
library(ggplot2)
```
```{r}
p <- ggplot(dfTrain, aes(y=as.factor(sexo), x=altura, fill=sexo)) + 
  geom_violin()
p
```

```{r}
alturasMenos165 = abs(dfTrain$altura - 165)
alturasMenos175 = abs(dfTrain$altura - 175)

```
```{r}
dfTrain$alt165 = alturasMenos165
dfTrain$alt175 = alturasMenos175
```

```{r}
dfT165 = dfTrain[order(dfTrain$alt165),]
dfT175 = dfTrain[order(dfTrain$alt175),]
```
```{r}
mean(dfT165$sexo[1:10])<1/2
mean(dfT175$sexo[1:10])<1/2
```
Entonces por voto de mayoria al de 165 sera clasificado mujer y al 175 como hombre
```{r}
promediosMoviles = function(A, S, alturaAestimar, h){
  df = data.frame(A,S)
  df2 = df[df$A > alturaAestimar-h & df$A < alturaAestimar+h ,]
  #print(df)
  if(mean(df2$S)>1/2 ){
    return(1)
  }
  return(0)
}
```

```{r}
promediosMoviles(dfTrain$altura, dfTrain$sexo, 170.5, 2.5)
```
```{r}
ClasificoVecinos =function(A,S,a,k){
  alturasMenosA = abs(A - a)
  dff = data.frame(S, alturasMenosA)
  dff = dff[order(dff$alturasMenosA),]
  if(mean(dff$S[1:k])<1/2){
    return(0)
  }
  return(1)

}
```

```{r}
ClasificoVecinos(dfTrain$altura, dfTrain$sexo, 166, 10)
```
```{r}
generarColumnaPrediccionesVecinos = function(dfTrain, dfEval, k){
  sexoPredicho = c()
  for (i in (1: length(dfEval$altura))){
    sexoPredicho[i] = ClasificoVecinos(dfTrain$altura, dfTrain$sexo, (dfEval$altura[i]), k)
  }
  #df = cbind(df, sexoPredicho)
  return(sexoPredicho)
}
```
```{r}
generarColumnaPrediccionesPromediosMoviles = function(dfTrain, dfEval , h){
  sexoPredicho = c()
  for (i in (1: length(dfEval$altura))){
    sexoPredicho[i] = promediosMoviles(dfTrain$altura, dfTrain$sexo, (dfEval$altura[i]), h)
  }
  return(sexoPredicho)
}
```

```{r}
vals = c(1:50)
res = c()
for(i in 1:length(vals)){
  aux = generarColumnaPrediccionesVecinos(dfTrain, dfTrain, vals[i])
  res[i] = sum(abs(dfTest$sexo - aux))
}
```
```{r}
valsH = c(1:30)
resP = c()
for(i in 1:length(valsH)){
  aux = generarColumnaPrediccionesPromediosMoviles(dfTrain, dfTrain, valsH[i])
  resP[i] = sum(abs(dfTest$sexo - aux))
}
```
```{r}
plot(vals,res, log="", col="red")
```

```{r}
#
plot(valsH, resP, col="blue")
```
```{r}
resP
```

```{r}
bws = c(0.1,0.5,1,2,3,4,5,6,10,15,20,30,40,50,80)
des = ksmooth(dfTrain$altura, dfTrain$sexo, bandwidth=2, x.points = dfTrain$altura)
```

```{r}
plot(des)
```
```{r}
density(x, )
```


```{r}
generarMatriz = function(datosX, funciones){
  matriz = matrix(0,  length(datosX), length(funciones))
  res =lapply(funciones, do.call, list((datosX)))
  for (i in 1:length(funciones)){
    vec = unlist(res[i])
    for (j in 1:length(datosX)){
      matriz[j,i] = vec[j]   
    }
  }
  return(matriz)
}
```

```{r}
modeloLinealizado = function(datosX, y, funciones){
  X = generarMatriz(datosX, funciones)
  transX = t(X)
  theta = solve(transX%*%X) %*% (transX) %*% (y)
  return(theta)
} 
```
```{r}
f0 = function(x) return(x**0)
f1 = function(x) return(x)
f2 = function(x) return(x**2)
funcionesPack = c(f0,f1,f2)
```

```{r}
#modeloLinealizado(datosX, y, funcionesPack)
```

```{r}
generarMallaEvaluar = function(ini, fin, delta, funciones, theta){
  malla = seq(ini, fin, delta)
  res =lapply(funciones, do.call, list(malla))
  ev = rep(0, length(malla))
  for (j in 1:length(malla)){
    for (i in 1:length(funciones)){
      ev[j] = ev[j] + theta[i]*unlist(res[i])[j]
    }
  }
  return(list(malla, ev))
}

```


```{r}
y = cars$dist
x = cars$speed
theta = modeloLinealizado(x, y, funcionesPack)
resu  = generarMallaEvaluar(4, 26, 0.01, funcionesPack, theta)
```
```{r}
#modR = lm(y ~ poly(x, 2))
#modR.predict(unlist(resu[1]))
#prediccion = predict(modR, data.frame( x = unlist(resu[1])))#$fit
```

```{r}
plot(cars$speed, cars$dist)
lines(unlist(resu[1]), unlist(resu[2]), col= "red")
#lines(unlist(resu[1]), prediccion, col= "blue")

```
```{r}
datosX = c(1,2,3,4,5,6,7,8,9,10,11,12)
y2 = c(1,4.2,9.1,16, 25.0, 37,24,18,9,5,2,-1)
theta2 = modeloLinealizado(datosX, y2, funcionesPack)
```



```{r}
mod = lm(data = df, unos ~ Tonnage + X + Time -1)
vif(mod)
```
```{r}
b = boxplot(rnorm(1000))
```

