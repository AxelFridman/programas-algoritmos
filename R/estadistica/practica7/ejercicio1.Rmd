---
title: "R Notebook"
output: html_notebook
---

```{r}
library(glmnet)
```

```{r}
set.seed(123)
repeticion <- function(n,p,seed,grilla,folds)
{
#grilla <- 10ˆseq(10,-2,length=100)
set.seed(seed)
betas_lineal <- c()
betas_ridge_min <- c()
betas_ridge_1se <- c()
betas_lasso_min <- c()
betas_lasso_1se <- c()
signif <- c()
for(i in 1:n)
{
# ----- #
# Acá defino las variables y covariables:
X1 <- runif(100,min = 0,max = 5)
X2 <- runif(100,min = 0,max = 5)
U3 <- runif(100,min = -0.1,max = 0.1)

U4 <- runif(100,min = -0.1,max = 0.1)
X3 <- 2*X1 + U3
X4 <- -X1 + U4
e <- rnorm(100)
Y <- 8*X1 - 5*X2 + X3 + 4*X4 + 5 + e
datos <- c()
datos <- cbind(datos,X1)
datos <- cbind(datos,X2)
datos <- cbind(datos,X3)
datos <- cbind(datos,X4)
datos <- cbind(datos,Y)
# ----- #
# Acá separo la muestra:
train_ind <- sample(c(TRUE,FALSE),size = 100,replace = TRUE,prob = c(p,1-p))
train <- datos[train_ind,]
test_ind <- !train_ind
test <- datos[test_ind,]
# ----- #
# Acá hago regresión lineal:
reg_1 <- lm(Y~.,data = as.data.frame(train))
betas_lineal <- rbind(betas_lineal,as.vector(reg_1$coefficients))
s <- summary(reg_1)
vector <- (as.vector(s$coefficients[,4]) <= 0.1)
signif <- rbind(signif,vector)
# ----- #
# Acá hago regrsión ridge:
reg_2 <- cv.glmnet(train[,-5],train[,5],alpha=0,lambda = grilla,nfolds = folds)
lrm <- reg_2$lambda.min
arm <- glmnet(train[,-5],train[,5],alpha = 0,lambda = lrm)
betas_ridge_min <- rbind(betas_ridge_min,as.vector(coef(arm)))
lr1 <- reg_2$lambda.1se

ar1 <- glmnet(train[,-5],train[,5],alpha = 0,lambda = lr1)
betas_ridge_1se <- rbind(betas_ridge_1se,as.vector(coef(ar1)))
# ----- #
# Acá hago regrsión lasso:
reg_3 <- cv.glmnet(train[,-5],train[,5],alpha=1,lambda = grilla,nfolds = folds)
llm <- reg_2$lambda.min
alm <- glmnet(train[,-5],train[,5],alpha = 1,lambda = llm)
betas_lasso_min <- rbind(betas_lasso_min,as.vector(coef(alm)))
ll1 <- reg_2$lambda.1se
al1 <- glmnet(train[,-5],train[,5],alpha = 1,lambda = ll1)
betas_lasso_1se <- rbind(betas_lasso_1se,as.vector(coef(al1)))
# ----- #
#print(100*i/n)
}
# ----- #
# Acá armo la matriz errores estándar:
matriz <- c()
matriz <- rbind(matriz,apply(betas_lineal,2,sd))
matriz <- rbind(matriz,apply(betas_ridge_min,2,sd))
matriz <- rbind(matriz,apply(betas_ridge_1se,2,sd))
matriz <- rbind(matriz,apply(betas_lasso_min,2,sd))
matriz <- rbind(matriz,apply(betas_lasso_1se,2,sd))
dimnames(matriz) <- list(c("Reg lineal","Ridge min","Ridge 1se","Lasso min","Lasso 1se"),c("b0","b1","b2","b3","b4"))

res <- t(as.matrix(apply(signif,2,mean)))*100
dimnames(res) <- list(c("%"),c("b0","b1","b2","b3","b4"))
print("-----------------------------------------------------------")
print("La proporción de veces que cada coeficiente fue significativo es:")
print(res)
print("-----------------------------------------------------------")
print("El devío estándar de cada coeficiente es:")
return(matriz)
}
```
```{r}
repeticion(1000,0.8,123,10^seq(10,-2,length=100),5)
```

```{r}
library("leaps")
```
```{r}
pk <- read.table("peak.txt",header = TRUE)
set.seed(12345)
logpk <- log(pk[,-1])
train_ind <- sample(c(TRUE,FALSE),size = length(logpk$y),replace = TRUE,prob = c(0.8,1-0.8))
train <- logpk[train_ind,]
test_ind <- !train_ind
test <- logpk[test_ind,]
Y <- train$y
X1 <- train$x1
X2 <- train$x2
X3 <- train$x3
X4 <- train$x4
X5 <- train$x5
X6 <- train$x6
X7 <- train$x7
X8 <- train$x8
X9 <- train$x9
reg <- lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9)
```

```{r}
mejores_modelos_forward <- regsubsets(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9, data = train,method = "forward",nvmax=10)
mejores <- summary(mejores_modelos_forward)
mejores
```

