---
title: "TP 4 validacion cruzada"
output: html_notebook
---

# Axel Fridman
```{r}
df = read.csv("datos_alquiler_crossvalidation.csv", stringsAsFactors = F)
df2 = read.csv("ar_properties.csv", stringsAsFactors = F)

```

1. Implemente una función que compute MAE:
```{r}
MAE=function(x,y){
  return(sum(abs(x-y))/length(x))
}
```
2. Implemente una función que compute PMAE:
```{r}
PMAE=function(x,y){
  return(sum(abs(x-y))/sum(abs(x)))
}
```
3. Considere el modelo que ajusta el precio en función de la superficie cubierta. Calcule el MAE y
PMAE. Agregue la variable fondo y compare.
```{r}
modleloPrecioSupCub = lm(df$price~df$surface_covered)
modleloPrecioSupCubyFondo= lm(df$price~df$surface_covered+df$fondo)

```
```{r}
mae1 = MAE(df$price, predict(modleloPrecioSupCub))
pmae1=PMAE(df$price, predict(modleloPrecioSupCub))
mae2= MAE(df$price, predict(modleloPrecioSupCubyFondo))
pmae2=PMAE(df$price, predict(modleloPrecioSupCubyFondo))
```
El MAE del primer modelo es `r mae1` mientras que el MAE del segundo es ligeramente menor siendo `r mae2`.


Por otro lado, los PMAE siendo una medida relativa a la escala tampoco varian mucho siendo respectivamente `r pmae1` y `r pmae2`

4. Construya una función crossval(datos, modelo, n_obs, fun_error, n_muestras=10) para calcular
el error promedio de predicción haciendo validación cruzada.
```{r}
crossval = function(datos, modelo, n_obs, fun_error, n_muestras=10){
  #¨df[sample(nrow(df), 3), ]
  errores = c()
  for(i in (1:n_muestras)){
    ind = sample(nrow(df),n_obs)
    
    datosEval = df[ind,]
    datosTrain = df[-ind, ]
    modeloActual = lm(modelo, data = datosTrain)
    
    #loPredecible = as.character(modelo[[2]])
    #print(datosEval$loPredecible)
    errores[i] = fun_error(datosEval$price, predict(modeloActual, datosEval))
  }
  #los errores obtenidos, el error promedio, su varianza, la formula
  #del modelo empleado y el modelo ajustado usando todos los datos.
  respuesta = list(errores, mean(errores), var(errores), modelo, lm(modelo, data = datos))
  return(respuesta)
}
```

5. Utilizando las funciones anteriores evalue el comportamiento de un modelo que ajuste el precio en
función de la superficie cubierta utilizando validación cruzada. Use como función de error al PMAE
```{r}
crossval(df,formula('price~surface_covered'), 10, PMAE, 10)
```
Parece que es un buen ajuste, en una escala de menos del 30% del valor real.

6. Considere el modelo que ajusta el precio en función de la superficie cubierta. 
Explore como varía el error al usar valores de n_obs iguales a distintos porcentajes del tamaño del dataset, por ejemplo
seq(1,100,5) y n_muestras=100. Grafique el error de validación cruzada en función de la cantidad de
observaciones separadas. Qué le indica esto sobre la cantidad de observaciones que debe usar para
validar el modelo?
```{r}
porcentaje = seq(1,100,5)
promedioErrorPorModelo = c()
for(i in 1:length(porcentaje)){
  promedioErrorPorModelo[i] = crossval(df,formula('price~surface_covered'), porcentaje[i], PMAE, 100)[[2]]
}
plot(porcentaje, promedioErrorPorModelo)
```
Como norma general, en el grafico se ve que aumenta exponencialmente el error promedio a medida que nos acercamos con el numero de observaciones al total del dataset.
Pero como tiene una parte de azar interesante por cada grafico ya que no controlamos el sample y hay saltitos cuando algun modelo justo se dio que era malo mas alla de la tendencia general, voy a "suavizarlo".
Como? Haciendo muchos modelos por cada porcentaje y tomando el promedio.
```{r}
porcentaje = seq(1,100,5)
promedioErrorPorModelo = c()
for(i in 1:length(porcentaje)){
  errorDeEstePorcentajeParticular = c()
  for(j in 1:10){
    errorDeEstePorcentajeParticular[j] = crossval(df,formula('price~surface_covered'), porcentaje[i], PMAE, 100)[[2]]
  }
  promedioErrorPorModelo[i] = mean(errorDeEstePorcentajeParticular)
}
plot(porcentaje, promedioErrorPorModelo)
```

7. Construya modelos usando las potencias de la variable fondo. Es decir, si p es el precio y f la el
fondo, considere modelos:
```{r}
modelosPotencia = c()
for (i in 1:8) {
  esc = paste('price~poly(fondo,', i, ')', sep="")
  modelosPotencia[i] = esc
}

errorModelos = c()
for(i in 1:8){
  errorDeEsteModeloParticular = c() #Suavizados los modelos
  for(j in 1:10){
    errorDeEsteModeloParticular[j] = crossval(df,modelosPotencia[i], round(length(df[,1])*0.2), PMAE, 20)[[2]]
  }
  errorModelos[i] = mean(errorDeEsteModeloParticular)
}
plot(1:8, errorModelos, log = 'y', xlab = 'Grado del modelo polinomial', ylab = 'Error promedio suavizado (escala log)')
```
El modelo con menor error de ajuste es el 1. Dicho esto, el crecimiento es incluso mas grande que exponencial simple, porque aun linealizando. El peor error lo tiene el de mayor grado, ya que el polinomio que me genera se memoriza los puntos donde entrena y es brutal la diferencia (debido a su alto crecimiento) con los puntos de evaluacion.

8. Compare la calidad de predicción de 3 modelos: precio en función de superficie cubierta, precio en
función de fondo y precio en función de ambas variables. Emplee en su comparación el error de ajuste
y el error de predicción.
```{r}
modPrecioSupCub = c()
modPrecioFon = c()
modPrecioSupyFondo = c()
gradoMax = 6
for (i in 1:gradoMax) {
  fon = paste('price~poly(fondo,', i, ')', sep="")
  sup = paste('price~poly(surface_covered,', i, ')', sep="")
  fonSup = paste('price~poly(fondo,', i, ') + ', paste('poly(surface_covered,', i, ')', sep=""), sep="")
  modPrecioFon[i] = fon
  modPrecioSupCub[i] = sup
  modPrecioSupyFondo[i] = fonSup
}

errorModeloSup = c()
errorModeloFon = c()
errorModeloSupFon = c()

for(i in 1:gradoMax){
  errorDeEsteModeloParticularSup = c() #Suavizados los modelos
  errorDeEsteModeloParticularFon = c()
  errorDeEsteModeloParticularSupFon = c()
  for(j in 1:10){
    errorDeEsteModeloParticularSup[j] = crossval(df,modPrecioSupCub[i], 1, PMAE, 20)[[2]]
    errorDeEsteModeloParticularFon[j] = crossval(df,modPrecioFon[i], 1, PMAE, 20)[[2]]
    errorDeEsteModeloParticularSupFon[j] = crossval(df,modPrecioSupyFondo[i], 1, PMAE, 20)[[2]]
  }
  errorModeloSup[i] = mean(errorDeEsteModeloParticularSup)
  errorModeloFon[i] = mean(errorDeEsteModeloParticularFon)
  errorModeloSupFon[i] = mean(errorDeEsteModeloParticularSupFon)
}

plot(1:gradoMax, errorModeloSup, col="magenta", ylim=range(errorModeloSup, errorModeloFon, errorModeloSupFon), pch=19, xlab = "Grado del modelo", ylab = "Error (PMAE) de cada modelo")
points(1:gradoMax, errorModeloFon, col="red",pch=19)
points(1:gradoMax, errorModeloSupFon, col="blue",pch=19)
lines(1:gradoMax, errorModeloSup, col="magenta", lty="dotted")
lines(1:gradoMax, errorModeloFon, col="red", lty="dotted")
lines(1:gradoMax, errorModeloSupFon, col="blue", lty="dotted")
legend(x="right", legend = c("Sup","Fon", "Sup+Fon"), pch = c(19,19,19), lty = "dotted", col = c("magenta", "red", "blue"))
```
Me parece muy interesante ver como el PMAE del modelo que toma la Superficie y el fondo casi sie,pre es peor en valores altos que el de solo superficie. Y el modelo del fondo esta alla arriba, volando en la estratosfera, que mal modelo.

9. Construya un programa que calcule el error de los 16 modelos posibles que incluyen las variables
superficie cubierta, fondo, tipo de propiedad y ubicación (considere las variables latitud y longitud
conjuntamente). Compare los modelos en términos de su error de ajuste y su error en validación
cruzada, usando un 20 % de los datos para validar el modelo. Cuál es el ranking entre los modelos?
Considerar más variables siempre mejora la predicción?

```{r}
modelosPosibles = c(formula('price~surface_covered'), 
                    formula('price~fondo'),
                    formula('price~property_type'),
                    formula('price~lat+lon'), 
                    formula('price~surface_covered+fondo'), 
                    formula('price~surface_covered+property_type'), 
                    formula('price~surface_covered+lat+lon'), 
                    formula('price~fondo+property_type'), 
                    formula('price~fondo+lat+lon'), 
                    formula('price~property_type+lat+lon'), 
                    formula('price~fondo+property_type+lat+lon'), 
                    formula('price~surface_covered+property_type+lat+lon'), 
                    formula('price~surface_covered+fondo+property_type'), 
                    formula('price~surface_covered+fondo+lat+lon'),
                    formula('price~surface_covered+fondo+property_type+lat+lon')
                    )
```
```{r}
indices = (df2$l1 == "Argentina" & 
  df2$l2 == "Capital Federal" &
    df2$price != 0 &
  df2$operation_type == "Alquiler" &
  ! is.na(df2$l3) &
  ! is.na(df2$lat) &
  ! is.na(df2$lon) &
  ! is.na(df2$property_type) &
  ! is.na(df2$surface_covered) &
  ! is.na(df2$price) &
  ! is.na(df2$surface_total)
    )
dfCasas = df2[df2$property_type=="Casa" & indices,]
dfCasas$fondo = dfCasas$surface_total - dfCasas$surface_covered
dfPH = df2[df2$property_type=="PH" & indices,]
dfPH$fondo = dfPH$surface_total - dfPH$surface_covered
```
```{r}
df = df[ , -which(names(df) %in% c("X"))]
dfPH = dfPH[,colnames(df)]
dfCasas = dfCasas[,colnames(df)]
df = rbind(df, dfPH[sample(nrow(dfPH), 97), ])
df = rbind(df, dfCasas[sample(nrow(dfCasas), 97), ])
```
Ahora, para poder analizar como influye en el modelo el property type, agregamos 97 PHs y 97 casas. Haciendo el dataset de tamaño 98*3 = 294. cuando antes era de tan solo 100 y la mayoria departamentos.
```{r}
errorModelos = c()
errorNaiveModelos = c()
for(i in 1:length(modelosPosibles)){
  errorDeEsteModeloParticular = c() #Suavizados los modelos
  for(j in 1:5){
    errorDeEsteModeloParticular[j] = crossval(df,modelosPosibles[[i]], 10, PMAE, 58)[[2]]
  }
  errorModelos[i] = mean(errorDeEsteModeloParticular)
  errorNaiveModelos[i] = PMAE(df$price,predict(lm(modelosPosibles[[i]], data=df), df))
}
names(errorModelos) = gsub("price", "", as.character(modelosPosibles))
par(mar=c(15, 3, 3, 1)) 
barplot(sort(errorModelos, decreasing = T), main="Validacion Cruzada errores", las=2, cex.names = 0.7,
        names.arg=names(sort(errorModelos, decreasing = T)), xlab = "", ylab = "Error PMAE")
```


Se puede ver como el error del modelo con mas variables explicativcas no siempre es el mejor, y a veces incluso resta. Por otro lado es muy facil apreciar dada una simplicidad del modelo, que variable explicativa es la mejor y cuan buena es. Surface covered es la mejor sola, pero el mejor modelo de todos en cuanto al error de validacion cruzada es el que lo tiene todo.
```{r}
errores = data.frame(1:15)
errores$cruzada = errorModelos
errores$naive = errorNaiveModelos
errores$modelo = names(errorModelos)
errores = errores[order(errores$cruzada, decreasing = T),]

par(mar=c(16, 3, 3, 1))
dev.new(width=5, height=25, unit="cm")
plot(c(1:length(errorModelos)), errores$cruzada, col="magenta", ylim=range(0,errorNaiveModelos, errorModelos,1),pch=19, xlab = "Modelo nro", ylab= "Errores" ,type="b")
lines(c(1:length(errorNaiveModelos)), errores$naive, col="green",pch=19)
points(c(1:length(errorNaiveModelos)), errores$naive, col="green",pch=19)


lines(errorModelos[1], errorNaiveModelos[1], col="magenta")
segments(x0 = c(1:length(errorNaiveModelos)),                   
         y0 = errores$cruzada,
         x1 = c(1:length(errorNaiveModelos)),
         y1 = errores$naive)

axis(1, at=1:length(errorModelos), labels=1:15, las=1)
legend(x="top" ,legend = c("Error cruzado","Error naive"), pch = c(19,19), col = c("magenta", "green"))
print(errores$modelo)
```
Es interesante notar que el error de validacion cruzada es siempre mayor que el error de ajuste. Pues claro, al ser entrenado con solo un subconjunto del dataset, es logico que tenga un peor ajuste cuando lo evaluo en los restantes, porque los desconoce. Mientras que el error de ajuste lo conoce todo.



